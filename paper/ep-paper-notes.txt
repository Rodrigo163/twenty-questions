[DECISION TREE CREATION]
  - this is a classification task, but it's different from standard DT classification tasks because the number of categories is the same as the number of instances
  - consequence: can't use the usual measures of entropy reduction/information gain like in the ID3 algorithm
    - those rely on being able to group instances together into classes
    - not possible here, since e.g. there's only one instance for the class "tortoise", only  one instance for the class "wren", etc.
    - for e.g. ID3 to work, there have to be many instances that correspond to "tortoise", "wren", etc.
  - could instead use the *cardinality* of the subsets
    - e.g. kn = `hopkins-knowledge.csv`, kn[kn['Hair']==1] gives a dataset that has 43 instances, and ==0 the other 57
    - one potential strategy:
      - get a ratio between ==0 and ==1, e.g. 57/43 = 0.75, and choose the feature with the ratio nearest to 1
      - motivation: the closer it is to 1, the more evenly split the dataset is by dividing on that feature
        - if ratio > 1, more ==0 than ==1
        - if 0 < ratio < 1, many more ==1 than ==0
      - in algorithm, will need to re-calculate this at every step in the decision tree
    - there might be other strategies too, but this one is what I'll implement
  - efficiently computing difference of this ratio from 1---is there a difference between methods?

    - method 1: apply to col, idxmin

      def dist_from_1(attrib_col):
        counts = attrib_col.value_counts()
        if len(counts) == 2:
            ratio = counts[0] / counts[1]
            return abs( 1 - ratio )
        return -1
      X.apply(dist_from_1, axis=0).idxmin()

      - 100 rows, 28 features: 35.6 ms ± 3.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 10 rows, 28 features: 33.2 ms ± 1.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 100 rows, 14 features: 19.8 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
      - 100 rows, 7 features: 11 ms ± 531 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

    - method 2: for loop on df, return feature

      def feat_nearest_1(df):
          curr_min = 10e10
          curr_min_feat = ''
          for feat in df.columns:
              counts = X[feat].value_counts()
              ratio = counts[0] / counts[1]
              dist_from_1 = abs( 1 - ratio )
              if dist_from_1 < curr_min:
                  curr_min = dist_from_1
                  curr_min_feat = feat
          return curr_min_feat

      - 100 rows, 28 features: 32.9 ms ± 1.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 10 rows, 28 features: 36.4 ms ± 3.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 100 rows, 14 features: 16.5 ms ± 430 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
      - 100 rows, 7 features: 10.3 ms ± 811 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

    - doesn't seem to matter too much which method I use, so I'll use the apply to col method because it's shorter
    - actually, this is better, because we might want rankings of which are the best to use at any given time
    - for example, if there are two that both have the same min distance to 1, could randomly choose between them (reduces determinism)
