[DECISION TREE CREATION]
  - this is a classification task, but it's different from standard DT classification tasks because the number of categories is the same as the number of instances
  - consequence: can't use the usual measures of entropy reduction/information gain like in the ID3 algorithm
    - those rely on being able to group instances together into classes
    - not possible here, since e.g. there's only one instance for the class "tortoise", only  one instance for the class "wren", etc.
    - for e.g. ID3 to work, there have to be many instances that correspond to "tortoise", "wren", etc.
  - could instead use the *cardinality* of the subsets
    - e.g. kn = `hopkins-knowledge.csv`, kn[kn['Hair']==1] gives a dataset that has 43 instances, and ==0 the other 57
    - one potential strategy:
      - get a ratio between ==0 and ==1, e.g. 57/43 = 0.75, and choose the feature with the ratio nearest to 1
      - motivation: the closer it is to 1, the more evenly split the dataset is by dividing on that feature
        - if ratio > 1, more ==0 than ==1
        - if 0 < ratio < 1, many more ==1 than ==0
      - in algorithm, will need to re-calculate this at every step in the decision tree
    - there might be other strategies too, but this one is what I'll implement
  - efficiently computing difference of this ratio from 1
    - method 1: apply to col, idxmin
      - 100 rows, 28 features: 35.6 ms ± 3.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 10 rows, 28 features: 33.2 ms ± 1.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 100 rows, 14 features: 19.8 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
      - 100 rows, 7 features: 11 ms ± 531 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

    - method 2: for loop on df, return feature
      - 100 rows, 28 features: 32.9 ms ± 1.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 10 rows, 28 features: 36.4 ms ± 3.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
      - 100 rows, 14 features: 16.5 ms ± 430 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
      - 100 rows, 7 features: 10.3 ms ± 811 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

    - doesn't seem to matter too much which method I use, so I'll use the apply to col method because it's shorter
