[REINFORCEMENT LEARNING]
  - "Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior" [1]{SuttonBarto2018}
  - "Reinforcement learning is learning what to do---how to map situations to actions---so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate [2] reward but also the next situation and, through that, all subsequent rewards. These two characteristics---trial-and-error search and delayed reward---are the two most important distinguishing features of reinforcement learning" [1--2]{SuttonBarto2018}
  - "We formalize the problem of reinforcement learning using ideas from dynamical systems theory, specifically, as the optimal control of incompletely-known Markov decision processes. ... the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal." [2]{SuttonBarto2018}
  - "Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior, reinforcement learning is trying to maximize a reward signal instead of trying to find hidden structure." [2]{SuttonBarto2018}
  - trade-off between exploration and exploitation: "To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward. But to discover such actions, it has to try actions that it has not elected before. The agent has to \textit{exploit} what it has already experienced in order to obtain reward, but it also has to \textit{explore} in order to make better action selections in the future." [3]{SuttonBarto2018}
  - tic-tac-toe example from {SuttonBarto2018}
    - "To select our moves we examine the states that would result from each of our possible moves (one for each blank space on the board) and look up their current values in the table [which we initialise]. Most of the time we move \textit{greedily}, selecting the move that leads to the state with greatest value, that is, with the highest estimated probability of winning. Occasionally, however, we select randomly from among the other moves instead. These are called \textit{exploratory} moves because they cause us to experience states that we might otherwise never see" [9]{SuttonBarto2018}
    - "While we are playing, we change the values of the states in which we find ourselves during the game. We attempt to make them more accurate estimates of the probabilities of winning. ... the current value of the earlier state is updated to be closer to the value of the later state ... V(S_t) \leftarrow V(S_t) + \alpha[V(S_{t+1} - V(S_t))]" [9]{SuttonBarto2018}
      - "\alpha is a small positive fraction called the \textit{step-size parameter}, which influences the rate of learning"
    - "It is a striking feature of the reinforcement learning solution that it can achieve the effects of planning and lookahead without using a model of the opponent and without conducting an explicit search over possible sequences of future states and actions" [11]{SuttonBarto2018}



  [COMPONENTS OF RL]

    [STATE/ENVIRONMENT]
      - RL "relies heavily on the concept of state---as input to the policy and value function, and as both input to and output from the model" [7]{SuttonBarto2018}
      - "we can think of the state as a signal conveying to the agent some sense of 'how the environment is' at a particular time" [7]{SuttonBarto2018}

    [AGENT]
      - "All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments" [3]{SuttonBarto2018}

    [POLICY]
      - "defines the learning agent's way of behaving at a given time" [6]{SuttonBarto2018}
      - "a mapping from perceived states of the environment to actions to be taken when in those states" [6]{SuttonBarto2018}
      - "In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process" [6]{SuttonBarto2018}
      - "policies may be stochastic, specifying probabilities for each action" [6]{SuttonBarto2018}

    [REWARD SIGNAL]
      - "defines the goal of a reinforcement learning problem" [6]{SuttonBarto2018}
      - "On each time step, the environment sends to the reinforcement learning agent a single number called the \textit{reward}. The agent's sole objective is to maximize the total reward it receives over the long run." [6]{SuttonBarto2018}
      - short-term, for the immediate time step
      - "the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future" [6]{SuttonBarto2018}
      - "may be stochastic functions of the state of the environment and the actions taken" [6]{SuttonBarto2018}

    [VALUE FUNCTION]
      - "Whereas the reward signal indicates what is good in an immediate sense, a \textit{value function} specifies what is good in the long run" [6]{SuttonBarto2018}
      - "the \textit{value} of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state" [6]{SuttonBarto2018}
      - "values indicate the \textit{long-term} desirability of states after taking into account the states that are likely to follow and the rewards available in those states" [6]{SuttonBarto2018}
      - "Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary ... Nevertheless, it is values with which we are most concerned when making and evaluating decisions" [6]{SuttonBarto2018}
      - "it is much harder to determien values than it is to determine rewards. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime" [6]{SuttonBarto2018}

    [MODEL OF THE ENVIRONMENT]
      - only in some RL systems
      - "mimics the behavior of the environment, or more generally, ... allows inferences to be made about how the environment will behave" [7]{SuttonBarto2018}
      - "For example, given a state and action, the model might predict the resultant next state and next reward" [7]{SuttonBarto2018}
      - "Methods for solving reinforcement learning problems that use models and planning are called \textit{model-based} methods, as opposed to simpler \textit{model-free} methods that are explicitly trial-and-error learners---viewed as almost the \textit{opposite} of planning" [7]{SuttonBarto2018}
      - which makes sense for us? which one are we going to use?
      - "Because models have to be reasonably accurate to be useful, model-free methods can have advantages over more complex methods when the real bottleneck in solving a problem is the difficulty of constructing a sufficiently accurate environment model" [12]{SuttonBarto2018}
